# AI Agent Assignment Learnings

## What Worked

**Agent Differentiation**: Advisor and novice clearly maintained distinct voices. The advisor gave comprehensive teaching content while the novice created structured study materials from the given guidance.

**Technical Accuracy**: Both agents demonstrated solid understanding of RL concepts (e.g. MDPs, value functions, policy gradients, exploration-exploitation, temporal difference learning). Technical content produced is interview-appropriate.

**Practical Utility**: Final study guide is useful - organized sections, practice questions, real-world examples, and confidence tips that would actually help in interviews.

## What Didn't Work

**Agent Interaction Visibility**: We cannot see the actual conversation between agents and just have the individual outputs. The mentor-mentee dynamic is not visible as a dialogue.

**Response Length Variation**: The advisor's interview prep was relatively brief compared to the novice's detailed study guide, creating imbalanced contributions.

**Persona Consistency**: While technically accurate, the responses felt somewhat generic rather than reflecting the specific personalities described in the backstories (e.g. the advisor's publications at NeurIPS/ICML, specific industry experience).

**Context Integration**: The novice's study guide didn't explicitly reference or build upon the advisor's specific guidance - it reads more like an independent creation.

## What I Learned

**AI Agent Capabilities**: AI agents can effectively simulate specialized roles and produce domain-specific content that's practically useful. They are particularly good at organizing complex information into structured formats.

**Limitations of Current Systems**: The agents don't engage in natural back-and-forth dialogue. Instead, they produce individual outputs in sequence, missing the interactive mentoring experience.

**Content Quality vs. Personalization**: While the technical content is solid, the agents struggle to maintain the specific personal details and experiences outlined in their backstories.

**Practical Applications**: This system could genuinely help with study preparation, research organization, and interview prep - the outputs are usable in real academic work.

**Task Design Importance**: The quality of agent output heavily depends on how well the tasks are structured. Clear expectations and context dependencies are crucial.

The two-agent approach successfully demonstrates how AI can assist with academic tasks, though it functions more like specialized content generators than conversational mentors.